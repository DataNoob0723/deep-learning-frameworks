{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data into memory\n",
    "with open(\"pickle_data/text_train.pickle\", \"rb\") as f:\n",
    "    text_train = pickle.load(f)\n",
    "\n",
    "with open(\"pickle_data/sent_train.pickle\", \"rb\") as f:\n",
    "    sent_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(text_train))\n",
    "print(len(sent_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data into memory\n",
    "with open(\"pickle_data/text_test.pickle\", \"rb\") as f:\n",
    "    text_test = pickle.load(f)\n",
    "\n",
    "with open(\"pickle_data/sent_test.pickle\", \"rb\") as f:\n",
    "    sent_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(text_test))\n",
    "print(len(sent_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the train data\n",
    "corpus_train = []\n",
    "for text in text_train:\n",
    "    text = text.replace(\"<br /><br />\", \" \") # Replace <br /><br /> with space\n",
    "    text = re.sub(r'\\W', ' ', text) # Replace punctuations with space\n",
    "    text = text.lower() # Conveter to lower case\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text) # Replace single characters with space\n",
    "    text = re.sub(r'^[a-z]\\s+', ' ', text) # Replace single characters at the beginning of the sentencecs with space\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    corpus_train.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero day leads you to think even re think why two boys young men would do what they did commit mutual suicide via slaughtering their classmates it captures what must be beyond bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction it is not perfect movie but given what money time the filmmaker and actors had it is remarkable product in terms of explaining the motives and actions of the two young suicide murderers it is better than elephant in terms of being film that gets under our rationalistic skin it is far far better film than almost anything you are likely to see flawed but honest with terrible honesty '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see the results of preprocessing\n",
    "corpus_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data\n",
    "corpus_test = []\n",
    "for text in text_test:\n",
    "    text = text.replace(\"<br /><br />\", \" \") # Replace <br /><br /> with space\n",
    "    text = re.sub(r'\\W', ' ', text) # Replace punctuations with space\n",
    "    text = text.lower() # Conveter to lower case\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text) # Replace single characters with space\n",
    "    text = re.sub(r'^[a-z]\\s+', ' ', text) # Replace single characters at the beginning of the sentencecs with space\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    corpus_test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.15232"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len = sum([len(e.split()) for e in corpus_train]) / len(corpus_train)\n",
    "avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to use Glove data\n",
    "\"\"\"\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        for w in sorted(words):\n",
    "            word_to_index[w] = i\n",
    "            index_to_word[i] = w\n",
    "            i += 1\n",
    "    \n",
    "    return word_to_index, index_to_word, word_to_vec_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map, words = read_glove_vecs('glove_data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "# Run a quick check\n",
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check word vector dimension\n",
    "word_to_vec_map[word].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding() \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    # Loop over training examples\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split() # Returns a list of words\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in words:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j += 1\n",
    "                if j >= max_len:\n",
    "                    break\n",
    "            \n",
    "    return X_indices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "# Run a quick check\n",
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1                # 0 is reserved for padding\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]    # 50\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(1, activation=None)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 300, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 300, 128)          91648     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,411\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_model((maxLen, ), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(corpus_train, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 1111s 44ms/step - loss: 0.6537 - acc: 0.5850\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.4465 - acc: 0.8021\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.4025 - acc: 0.8234\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 1106s 44ms/step - loss: 0.3826 - acc: 0.8332\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 1107s 44ms/step - loss: 0.3633 - acc: 0.8424\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 1106s 44ms/step - loss: 0.3411 - acc: 0.8550\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 1107s 44ms/step - loss: 0.3283 - acc: 0.8598\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 1106s 44ms/step - loss: 0.3109 - acc: 0.8704\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 1107s 44ms/step - loss: 0.2972 - acc: 0.8762\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 1106s 44ms/step - loss: 0.2778 - acc: 0.8848\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.2678 - acc: 0.8910\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.2441 - acc: 0.8994\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.2269 - acc: 0.9088\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.2108 - acc: 0.9167\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.1892 - acc: 0.9280\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.1708 - acc: 0.9357\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 1107s 44ms/step - loss: 0.1512 - acc: 0.9454\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 1110s 44ms/step - loss: 0.1376 - acc: 0.9516\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.1205 - acc: 0.9587\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.1095 - acc: 0.9626\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.1472 - acc: 0.9479\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 1112s 44ms/step - loss: 0.0972 - acc: 0.9679\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0766 - acc: 0.9760\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 1110s 44ms/step - loss: 0.0806 - acc: 0.9743\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0750 - acc: 0.9771\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0760 - acc: 0.9776\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0639 - acc: 0.9814\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 1112s 44ms/step - loss: 0.0744 - acc: 0.9772\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 1120s 45ms/step - loss: 0.0596 - acc: 0.9842\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 1127s 45ms/step - loss: 0.0646 - acc: 0.9811\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 1119s 45ms/step - loss: 0.0570 - acc: 0.9843\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 1172s 47ms/step - loss: 0.0610 - acc: 0.9822\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 1119s 45ms/step - loss: 0.0526 - acc: 0.9854\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 1110s 44ms/step - loss: 0.0467 - acc: 0.9878\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.0530 - acc: 0.9860\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 1111s 44ms/step - loss: 0.0484 - acc: 0.9873\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0489 - acc: 0.9867\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 1110s 44ms/step - loss: 0.0506 - acc: 0.9858\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 1112s 44ms/step - loss: 0.0449 - acc: 0.9883\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 1111s 44ms/step - loss: 0.0423 - acc: 0.9890\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 1112s 44ms/step - loss: 0.0410 - acc: 0.9893\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0415 - acc: 0.9893\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0432 - acc: 0.9888\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 1110s 44ms/step - loss: 0.0373 - acc: 0.9906\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0402 - acc: 0.9897\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 1108s 44ms/step - loss: 0.0420 - acc: 0.9891\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 1130s 45ms/step - loss: 0.0460 - acc: 0.9879\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 1118s 45ms/step - loss: 0.0366 - acc: 0.9902\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 1114s 45ms/step - loss: 0.0371 - acc: 0.9902\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 1109s 44ms/step - loss: 0.0349 - acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4ec205c0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, sent_train, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy on test set\n",
    "X_test_indices = sentences_to_indices(corpus_test, word_to_index, maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
