{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import necessary dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check the version of TensorFlow\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0, 10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "# Create dataset object from numpy array\n",
    "dx = tf.data.Dataset.from_tensor_slices(x)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-shot iterator which can only be initialized and run once\n",
    "iterator = dx.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Extract an element\n",
    "next_element = iterator.get_next()\n",
    "print(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  for i in range(10):\n",
    "    val = sess.run(next_element)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to repeatedly extract data from a dataset\n",
    "iterator = dx.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(15):\n",
    "        val = sess.run(next_element)\n",
    "        print(val)\n",
    "        if i % 9 == 0 and i > 0:\n",
    "            sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using batches\n",
    "dx = tf.data.Dataset.from_tensor_slices(x).batch(3)\n",
    "iterator = dx.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(15):\n",
    "        val = sess.run(next_element)\n",
    "        print(val)\n",
    "        if (i + 1) % (10 // 3) == 0 and i > 0:\n",
    "            sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zip together datasets. \n",
    "This is useful when pairing up input-output training/validation pairs of data (i.e. input images and matching labels for each image). \n",
    "\"\"\"\n",
    "def simple_zip_example():\n",
    "    x = np.arange(0, 10)\n",
    "    y = np.arange(1, 11)\n",
    "    # Create dataset objects from the arrays\n",
    "    dx = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dy = tf.data.Dataset.from_tensor_slices(y)\n",
    "    # Zip the two datasets together\n",
    "    dcomb = tf.data.Dataset.zip((dx, dy)).batch(3)\n",
    "    iterator = dcomb.make_initializable_iterator()\n",
    "    # Extract an element\n",
    "    next_element = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        for i in range(15):\n",
    "            val = sess.run(next_element)\n",
    "            print(val)\n",
    "            if (i + 1) % (10 // 3) == 0 and i > 0:\n",
    "                sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n"
     ]
    }
   ],
   "source": [
    "simple_zip_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can also do it repeatly\n",
    "def simple_zip_example2():\n",
    "    x = np.arange(0, 10)\n",
    "    y = np.arange(1, 11)\n",
    "    # Create dataset objects from the arrays\n",
    "    dx = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dy = tf.data.Dataset.from_tensor_slices(y)\n",
    "    # Zip the two datasets together\n",
    "    dcomb = tf.data.Dataset.zip((dx, dy)).repeat().batch(3)\n",
    "    iterator = dcomb.make_initializable_iterator()\n",
    "    # Extract an element\n",
    "    next_element = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        for i in range(15):\n",
    "            val = sess.run(next_element)\n",
    "            print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([9, 0, 1]), array([10,  1,  2]))\n",
      "(array([2, 3, 4]), array([3, 4, 5]))\n",
      "(array([5, 6, 7]), array([6, 7, 8]))\n",
      "(array([8, 9, 0]), array([ 9, 10,  1]))\n",
      "(array([1, 2, 3]), array([2, 3, 4]))\n",
      "(array([4, 5, 6]), array([5, 6, 7]))\n",
      "(array([7, 8, 9]), array([ 8,  9, 10]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([9, 0, 1]), array([10,  1,  2]))\n",
      "(array([2, 3, 4]), array([3, 4, 5]))\n"
     ]
    }
   ],
   "source": [
    "simple_zip_example2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple Neural Network example\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "digits = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "train_images = digits[0][:int(len(digits[0]) * 0.8)]\n",
    "train_labels = digits[1][:int(len(digits[0]) * 0.8)]\n",
    "valid_images = digits[0][int(len(digits[0]) * 0.8):]\n",
    "valid_labels = digits[1][int(len(digits[0]) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training datasets\n",
    "dx_train = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "# Apply a one-hot transformation to each label for use in the neural network\n",
    "dy_train = tf.data.Dataset.from_tensor_slices(train_labels).map(lambda z: tf.one_hot(z, 10))\n",
    "# Zip the x and y training data together and shuffle, batch etc.\n",
    "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same operations for the validation set\n",
    "dx_valid = tf.data.Dataset.from_tensor_slices(valid_images)\n",
    "dy_valid = tf.data.Dataset.from_tensor_slices(valid_labels).map(lambda z: tf.one_hot(z, 10))\n",
    "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float64, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(64)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "# Create general iterator\n",
    "print(train_dataset.output_types)\n",
    "print(train_dataset.output_shapes)\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "validation_init_op = iterator.make_initializer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple Neural Network Model\n",
    "def nn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    fc1 = tf.layers.dense(bn, 50)\n",
    "    fc2 = tf.layers.dense(fc1, 50)\n",
    "    fc2 = tf.layers.dropout(fc2)\n",
    "    fc3 = tf.layers.dense(fc2, 10)\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The next_element operation, because it is operating on the generic iterator which is defined by the shape of the train_dataset, \n",
    "is a tuple – the first element ([0]) will contain the MNIST images, while the second element ([1]) will contain the corresponding labels.\n",
    "\"\"\"\n",
    "logits = nn_model(next_element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 368.2855408171384, training accuracy: 0.10000000149011612\n",
      "Epoch: 50, loss: 8.232498104386487, training accuracy: 0.9333333373069763\n",
      "Epoch: 100, loss: 5.240766030038692, training accuracy: 0.9666666388511658\n",
      "Epoch: 150, loss: 4.337822829377826, training accuracy: 0.9333333373069763\n",
      "Epoch: 200, loss: 1.2398580430939572, training accuracy: 1.0\n",
      "Epoch: 250, loss: 3.290698012202787, training accuracy: 0.9666666388511658\n",
      "Epoch: 300, loss: 1.3161388262902953, training accuracy: 1.0\n",
      "Epoch: 350, loss: 2.0153298106564628, training accuracy: 0.9666666388511658\n",
      "Epoch: 400, loss: 3.1704537747950354, training accuracy: 0.9666666388511658\n",
      "Epoch: 450, loss: 2.732271721085435, training accuracy: 0.9666666388511658\n",
      "Epoch: 500, loss: 1.865572943875488, training accuracy: 0.9666666388511658\n",
      "Epoch: 550, loss: 4.0344662773575415, training accuracy: 0.9333333373069763\n",
      "Average validation set accuracy over 99 iterations is 0.8966666567325586\n"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "epochs = 600\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch: {i}, loss: {l}, training accuracy: {acc}\")\n",
    "    # Now set up the validation run\n",
    "    valid_iters = 100\n",
    "    # Re-initialize the iterator, but this time with validation data\n",
    "    sess.run(validation_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0] / valid_iters\n",
    "    print(f\"Average validation set accuracy over {i} iterations is {avg_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
