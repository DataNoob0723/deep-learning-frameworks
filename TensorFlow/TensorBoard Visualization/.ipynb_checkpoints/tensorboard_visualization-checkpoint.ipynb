{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return a batch of data for training\n",
    "def get_batch(X_data, Y_data, batch_size):\n",
    "    idxs = np.random.randint(0, len(Y_data), batch_size)\n",
    "    return X_data[idxs, :, :], Y_data[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define our Neural Network\n",
    "\"\"\"\n",
    "# Hyperparameters\n",
    "learning_rate = 0.5\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "# Network structure\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # Declare the training data placeholders\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "    # Reshape input X - for 28 x 28 pixels = 784\n",
    "    X_rs = tf.reshape(X, [-1, 784])\n",
    "    # scale the input data (maximum is 255.0, minimum is 0.0)\n",
    "    X_sc = tf.div(X_rs, 255.0)\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    Y = tf.placeholder(tf.int64, [None, 1])\n",
    "    # convert the y data to one hot values\n",
    "    Y_one_hot = tf.reshape(tf.one_hot(Y, 10), [-1, 10])\n",
    "\n",
    "with tf.name_scope(\"layer_1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W')\n",
    "    b1 = tf.Variable(tf.random_normal([300]), name='b')\n",
    "    hidden_logits = tf.add(tf.matmul(X_sc, W1), b1)\n",
    "    hidden_out = tf.nn.sigmoid(hidden_logits)\n",
    "\n",
    "with tf.name_scope(\"layer_2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.05), name='W')\n",
    "    b2 = tf.Variable(tf.random_normal([10]), name='b')\n",
    "    logits = tf.add(tf.matmul(hidden_out, W2), b2)\n",
    "\n",
    "\n",
    "# now let's define the cost function which we are going to train the model on\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_one_hot,\n",
    "                                                                          logits=logits))\n",
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Initialize\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_one_hot, 1), tf.argmax(logits, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.variable_scope(\"getimages\"):\n",
    "    correct_inputs = tf.boolean_mask(X_sc, correct_prediction)\n",
    "    image_summary_true = tf.summary.image('correct_images', tf.reshape(correct_inputs, (-1, 28, 28, 1)),\n",
    "                                          max_outputs=5)\n",
    "    incorrect_inputs = tf.boolean_mask(X_sc, tf.logical_not(correct_prediction))\n",
    "    image_summary_false = tf.summary.image('incorrect_images', tf.reshape(incorrect_inputs, (-1, 28, 28, 1)),\n",
    "                                           max_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'acc_summary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a summary to store the accuracy\n",
    "tf.summary.scalar('acc_summary', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more summaries\n",
    "tf.summary.histogram(\"Hidden_logits\", hidden_logits)\n",
    "tf.summary.histogram(\"Hidden_output\", hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store\n",
    "STORE_PATH = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, cost=0.6048911273479461, test set accuracy=0.9085999727249146\n",
      "Epoch: 2, cost=0.2778373076890906, test set accuracy=0.9279999732971191\n",
      "Epoch: 3, cost=0.2361723662912846, test set accuracy=0.9380999803543091\n",
      "Epoch: 4, cost=0.19849984689926115, test set accuracy=0.9466000199317932\n",
      "Epoch: 5, cost=0.16954776801789798, test set accuracy=0.9521999955177307\n",
      "Epoch: 6, cost=0.14840157803148027, test set accuracy=0.9563000202178955\n",
      "Epoch: 7, cost=0.1286307939141988, test set accuracy=0.9610999822616577\n",
      "Epoch: 8, cost=0.11776743534641956, test set accuracy=0.9652000069618225\n",
      "Epoch: 9, cost=0.10540059767353036, test set accuracy=0.9652000069618225\n",
      "Epoch: 10, cost=0.0952839459323634, test set accuracy=0.9682000279426575\n",
      "Epoch: 11, cost=0.08530482215496413, test set accuracy=0.9692999720573425\n",
      "Epoch: 12, cost=0.08087235528354836, test set accuracy=0.9682000279426575\n",
      "Epoch: 13, cost=0.07282382933422916, test set accuracy=0.9726999998092651\n",
      "Epoch: 14, cost=0.07069023152968541, test set accuracy=0.9732999801635742\n",
      "Epoch: 15, cost=0.06435688765874749, test set accuracy=0.9735999703407288\n",
      "Epoch: 16, cost=0.05871407833726455, test set accuracy=0.9745000004768372\n",
      "Epoch: 17, cost=0.05602384846967952, test set accuracy=0.9751999974250793\n",
      "Epoch: 18, cost=0.052586749637654674, test set accuracy=0.975600004196167\n",
      "Epoch: 19, cost=0.04776385945112756, test set accuracy=0.9769999980926514\n",
      "Epoch: 20, cost=0.04705993532513579, test set accuracy=0.9782000184059143\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Start the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(STORE_PATH, sess.graph)\n",
    "    \n",
    "    total_batch = int((len(Y_train) / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_X, batch_Y = get_batch(X_train, Y_train, batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={X: batch_X, Y: batch_Y.reshape(-1, 1)})\n",
    "            avg_cost += c / total_batch\n",
    "        acc, summary = sess.run([accuracy, merged], feed_dict={X: X_test, Y: Y_test.reshape(-1, 1)})\n",
    "        print(f\"Epoch: {epoch + 1}, cost={avg_cost}, test set accuracy={acc}\")\n",
    "        writer.add_summary(summary, epoch)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "TensorBoard 1.9.0 at http://zeyudeMacBook-Pro.local:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# To run TensorBoard, run the following command in terminal\n",
    "!tensorboard --logdir=STORE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
